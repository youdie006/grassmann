{
  "run_dir": "/tmp/grassmann-exp/training_results_hydra/multirun/2026-02-27/20-23-53/0_attention",
  "experiment_name": "paper_6l_128",
  "strategy": "attention",
  "num_params": 17356800,
  "best_val_loss": 7.230816413616312,
  "best_val_ppl": 1381.349796638809,
  "train_losses": [
    13.053191714533298,
    7.549713705950242,
    7.324521499096604,
    7.263273685373725,
    7.2447987860749326,
    7.2333586109725765,
    7.225151807441644,
    7.218540277498079,
    7.2152007378358896,
    7.21267109971208,
    7.20995690945848,
    7.258761316697227,
    7.206664307232209,
    7.2052891343672645,
    7.203679640662862,
    7.202382915585224,
    7.200633039661482,
    7.199369821531462,
    7.197978214359114,
    7.196594963311724,
    7.195624588645078,
    7.195062636479125,
    7.193982179577126,
    7.192277009899391,
    7.1912540879479065,
    7.190543394037746,
    7.19017029913564,
    7.189237425459048,
    7.188977341813412,
    7.189620323147153
  ],
  "val_losses": [
    7.903616543473868,
    7.4587487845585265,
    7.3001144672262255,
    7.2878948737835065,
    7.2619179692761655,
    7.245932151531351,
    7.253662109375,
    7.250953345463194,
    7.242942349664096,
    7.244309458239325,
    7.24310197501347,
    7.242480837065598,
    7.239695187272696,
    7.242790616791824,
    7.238976708773909,
    7.235150435875202,
    7.240358747284988,
    7.239308455894733,
    7.237891756255051,
    7.236257487329944,
    7.23523291226091,
    7.232232850173424,
    7.234779094827586,
    7.234507396303374,
    7.230816413616312,
    7.232681669037918,
    7.234506015119882,
    7.233402515279836,
    7.233022360966124,
    7.2348547310664735
  ],
  "config": {
    "model": {
      "tensor_lifting_strategy": "attention",
      "num_layers": 12,
      "d_model": 256,
      "num_heads": 4,
      "expansion_ratio": 4,
      "d_low": 32,
      "lags": [
        1,
        1,
        2,
        2,
        4,
        4,
        8,
        8,
        12,
        12,
        16,
        16
      ],
      "pre_norm": false,
      "weight_tying": true,
      "layerwise_lags": true,
      "use_custom_init": false
    },
    "data": {
      "source": "local_parquet",
      "data_root": "/tmp/wikitext2_official/wikitext-2-raw-v1",
      "tokenizer_name": "bert-base-uncased",
      "max_context_len": 256,
      "max_samples_train": null,
      "max_samples_val": null
    },
    "seed": 42,
    "train": {
      "device": "cuda",
      "batch_size": 16,
      "num_epochs": 30,
      "learning_rate": 0.001,
      "weight_decay": 0.01,
      "grad_clip": 1.0,
      "dropout_rate": 0.1,
      "num_workers": 0,
      "pin_memory": true,
      "amp": true
    },
    "experiment": {
      "name": "paper_6l_128",
      "run_id": null,
      "output_root": "training_results_hydra"
    }
  },
  "lm_config": {
    "vocab_size": 30522,
    "max_context_len": 256,
    "num_layers": 12,
    "tensor_lifting_strategy": "attention",
    "lags": [
      1,
      1,
      2,
      2,
      4,
      4,
      8,
      8,
      12,
      12,
      16,
      16
    ],
    "d_model": 256,
    "num_heads": 4,
    "expansion_ratio": 4,
    "dropout_rate": 0.1,
    "weight_tying": true,
    "d_low": 32,
    "pre_norm": false,
    "layerwise_lags": true,
    "use_custom_init": false
  }
}