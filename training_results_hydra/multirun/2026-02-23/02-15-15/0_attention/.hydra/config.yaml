model:
  tensor_lifting_strategy: attention
  num_layers: 6
  d_model: 256
  num_heads: 4
  expansion_ratio: 4
  d_low: 32
  lags:
  - 1
  - 2
  - 4
  - 8
  - 12
  - 16
  pre_norm: false
  weight_tying: true
data:
  source: local_parquet
  data_root: ${hydra:runtime.cwd}/analysis/data/wikitext2_official/wikitext-2-raw-v1
  tokenizer_name: bert-base-uncased
  max_context_len: 128
  max_samples_train: null
  max_samples_val: null
seed: 42
train:
  device: auto
  batch_size: 32
  num_epochs: 1
  learning_rate: 0.001
  weight_decay: 0.01
  grad_clip: 1.0
  dropout_rate: 0.1
  num_workers: 2
  pin_memory: true
  amp: true
experiment:
  name: paper_6l_128
  run_id: null
  output_root: training_results_hydra
