{
  "run_dir": "/tmp/grassmann-exp/training_results_hydra/multirun/2026-02-23/14-13-53/0_attention",
  "experiment_name": "paper_6l_128",
  "strategy": "attention",
  "num_params": 17356800,
  "best_val_loss": 5.385780745539172,
  "best_val_ppl": 218.28045909159343,
  "train_losses": [
    6.75707672921519,
    6.084916575494722,
    5.7416547768468735,
    5.459702061671837,
    5.203527878100009,
    4.975401971004438,
    4.767955183345366,
    4.579556630894462,
    4.408376896870115,
    4.253094063723151,
    4.107416688864669,
    3.972757610195248,
    3.8489212518069835,
    3.7319232457036855,
    3.621932005924762,
    3.519033607527109,
    3.422453500370291,
    3.332656290016922,
    3.2502310769018217,
    3.173434685895787,
    3.106239158626971,
    3.0453489918972294,
    2.99155034865925,
    2.946871051601335,
    2.9079897199722535,
    2.8780102547052286,
    2.852671863348514,
    2.837041701862519,
    2.8263140336714963,
    2.823312859696713
  ],
  "val_losses": [
    6.359020800426088,
    6.076561574278207,
    5.89720525412724,
    5.735629838088463,
    5.6129351237724565,
    5.519869771497003,
    5.449489848367099,
    5.409312790837781,
    5.39790206119932,
    5.385780745539172,
    5.392820563809625,
    5.421684807744519,
    5.446514491377206,
    5.4915485299866775,
    5.535058506603899,
    5.576388712587027,
    5.62831555563828,
    5.6759071021244445,
    5.725295971179831,
    5.776865227469083,
    5.812789135965808,
    5.850876577969255,
    5.886238805178938,
    5.914269143137439,
    5.93579571822594,
    5.950143329028426,
    5.960441770224736,
    5.969115808092314,
    5.970489271755876,
    5.971394078484897
  ],
  "config": {
    "model": {
      "tensor_lifting_strategy": "attention",
      "num_layers": 12,
      "d_model": 256,
      "num_heads": 4,
      "expansion_ratio": 4,
      "d_low": 32,
      "lags": [
        1,
        1,
        2,
        2,
        4,
        4,
        8,
        8,
        12,
        12,
        16,
        16
      ],
      "pre_norm": true,
      "weight_tying": true,
      "layerwise_lags": true
    },
    "data": {
      "source": "local_parquet",
      "data_root": "/tmp/wikitext2_official/wikitext-2-raw-v1",
      "tokenizer_name": "bert-base-uncased",
      "max_context_len": 256,
      "max_samples_train": null,
      "max_samples_val": null
    },
    "seed": 42,
    "train": {
      "device": "cuda",
      "batch_size": 16,
      "num_epochs": 30,
      "learning_rate": 0.001,
      "weight_decay": 0.01,
      "grad_clip": 1.0,
      "dropout_rate": 0.1,
      "num_workers": 0,
      "pin_memory": true,
      "amp": true
    },
    "experiment": {
      "name": "paper_6l_128",
      "run_id": null,
      "output_root": "training_results_hydra"
    }
  },
  "lm_config": {
    "vocab_size": 30522,
    "max_context_len": 256,
    "num_layers": 12,
    "tensor_lifting_strategy": "attention",
    "lags": [
      1,
      1,
      2,
      2,
      4,
      4,
      8,
      8,
      12,
      12,
      16,
      16
    ],
    "d_model": 256,
    "num_heads": 4,
    "expansion_ratio": 4,
    "dropout_rate": 0.1,
    "weight_tying": true,
    "d_low": 32,
    "pre_norm": true,
    "layerwise_lags": true
  }
}