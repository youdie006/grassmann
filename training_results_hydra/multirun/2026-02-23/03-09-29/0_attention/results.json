{
  "run_dir": "/mnt/d/MyProject/AINWYN/analysis/repos/kanenorman-grassmann/training_results_hydra/multirun/2026-02-23/03-09-29/0_attention",
  "experiment_name": "paper_6l_128",
  "strategy": "attention",
  "num_params": 12585472,
  "best_val_loss": 5.475257413140659,
  "best_val_ppl": 238.7119066042222,
  "train_losses": [
    12.875780741089358,
    6.8741653396319835,
    6.37348899858369,
    6.041778850214213,
    5.800899064604839,
    5.612047455605113,
    5.457766672792929,
    5.322191278495175,
    5.205363431428967,
    5.09966069854436,
    5.005521517533523,
    4.918710361439768,
    4.839594155176807,
    4.766866886125268,
    4.700703793220315,
    4.637838222046445,
    4.578849886312468,
    4.523330090084315,
    4.473653757508198,
    4.427587153446695,
    4.385756823585797,
    4.348793673813876,
    4.314849087931815,
    4.287248300951581,
    4.263511643640044,
    4.244815190917477,
    4.230780658653682,
    4.219984144558847,
    4.211886077532828,
    4.209367583177597
  ],
  "val_losses": [
    7.339582698098544,
    6.670282668080823,
    6.3167005818465665,
    6.118520078987911,
    5.951352308536398,
    5.863531252433514,
    5.7700206657935835,
    5.702196976234173,
    5.664723971794391,
    5.5971643924713135,
    5.5683529459196945,
    5.548515180061603,
    5.512210582864696,
    5.498617533979745,
    5.483081760077641,
    5.480524375520903,
    5.483490319087587,
    5.480807896318106,
    5.475257413140659,
    5.493405235224757,
    5.510544908457789,
    5.50297239731098,
    5.4911213989915515,
    5.512392915528396,
    5.512896718650029,
    5.5198367628557925,
    5.527948823468439,
    5.526332287952818,
    5.530887916170317,
    5.531568502557689
  ],
  "config": {
    "model": {
      "tensor_lifting_strategy": "attention",
      "num_layers": 6,
      "d_model": 256,
      "num_heads": 4,
      "expansion_ratio": 4,
      "d_low": 32,
      "lags": [
        1,
        2,
        4,
        8,
        12,
        16
      ],
      "pre_norm": false,
      "weight_tying": true
    },
    "data": {
      "source": "local_parquet",
      "data_root": "/mnt/d/MyProject/AINWYN/analysis/repos/kanenorman-grassmann/../../data/wikitext2_official/wikitext-2-raw-v1",
      "tokenizer_name": "bert-base-uncased",
      "max_context_len": 128,
      "max_samples_train": null,
      "max_samples_val": null
    },
    "seed": 42,
    "train": {
      "device": "auto",
      "batch_size": 32,
      "num_epochs": 30,
      "learning_rate": 0.001,
      "weight_decay": 0.01,
      "grad_clip": 1.0,
      "dropout_rate": 0.1,
      "num_workers": 2,
      "pin_memory": true,
      "amp": true
    },
    "experiment": {
      "name": "paper_6l_128",
      "run_id": null,
      "output_root": "training_results_hydra"
    }
  },
  "lm_config": {
    "vocab_size": 30522,
    "max_context_len": 128,
    "num_layers": 6,
    "tensor_lifting_strategy": "attention",
    "lags": [
      1,
      2,
      4,
      8,
      12,
      16
    ],
    "d_model": 256,
    "num_heads": 4,
    "expansion_ratio": 4,
    "dropout_rate": 0.1,
    "weight_tying": true,
    "d_low": 32,
    "pre_norm": false
  }
}